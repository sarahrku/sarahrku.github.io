#  Clustering

## Indledning og læringsmålene

### Læringsmålene

I skal være i stand til at 

* Beskrive hvad k-means clustering går ud på
* Anvende `kmeans` og output resultatet på en __tidy__ måde
* Iterate over forskellige antal clusters og vælge antallet som passer til de data
* Anvende funktionen `hclust` for at lave et simpel hierarchical clustering

### Inledning til chapter

I clustering er der til formål at dele et datasæt op i forskellige grupper (clusters eller klynge på dansk) af observationer, der mest ligner hinanden. Det øger indsigten i datasættet ved at fk. bedre forstår strukturen. De spørgsmål som vi kan prøve at give svar på er bla.:

* Hvor mange forskellige clusters er repræsenteret i mit datasæt?
* Hvilke individuelle observationer tilhører hvilken cluster?

I dette kapitel ser vi hvordan vi kan implementere både k-means clustering og hierarchical clustering i R (indenfor den tidyverse ramme), og bruge dem til at tage beslutninger om de ovenstående spørgsmål.

### Video ressourcer

* Video 1: K-means clustering 

Link her hvis det ikke virker nedenunder: https://player.vimeo.com/video/553656150
```{r,echo=FALSE}
library("vembedr")

embed_url("https://vimeo.com/553656150")
```


* Video 2: augment, glanced og tidy med K-means

Link her hvis det ikke virker nedenunder: https://player.vimeo.com/video/553656139
```{r,echo=FALSE}
library("vembedr")

embed_url("https://vimeo.com/553656139")
```


* Video 3: Hvor mange clusters skal man vælge?

Link her hvis det ikke virker nedenunder: https://player.vimeo.com/video/553656129
```{r,echo=FALSE}
library("vembedr")

embed_url("https://vimeo.com/553656129")
```



## K-means clustering

```{r,message=FALSE,comment=FALSE,warning=FALSE}
library(palmerpenguins)
library(tidyverse)
library(broom)
```

I k-means er alle observationer eller datapunkter tilknyttet til den nærmeste cluster, efter deres nærhed til hinanden. Man specificere i forvejen antallet af clusters som data skal være delt op ind i. Derfor, skal der være nogle undersøgelses arbejde for at vælge den bedste antal clusters som passer til problemstillingen/ bedste repræsenterer de data.

Lad os tage udgangspunkt i datasættet `penguins`. Vi begynder med at få fjernet observationerne med `NA` i mindst én variable med funktionen `drop_na` og ved at specificere at `year` skal være en faktor (for at skelne den fra de andre numeriske kolonner):

```{r}
data(penguins)
penguins <- penguins %>% 
  mutate(year=as.factor(year)) %>%
  drop_na() 
```

Vi vide allerede i forvejen, at der er 3 `species` med i de data, som vi plotter her med forskellige farver.

```{r,fig.width=5,fig.height=4}
penguins %>% ggplot(aes(x=bill_length_mm,y=body_mass_g,colour=species)) + 
  geom_point() + 
  theme_classic()
```

Vi vil gerne bruge k-means clustering på de numeriske variabler i de datasæt, og derefter kan det være nyttigt at sammenligne de clusters vi få med de tre species - hvor gode er de clusters til at skelne i mellem de forskellige species, eller fanger de noget andet struktur i de data (for eksempel kønnet eller øen, de bor)?

### Hvordan fungere kmeans?

K-means er en iterativ process. Lad os forestille os at vi gerne vil have tre clusters i de data. Man starter med tre observationer ved tilfælde og kalde dem for de cluster middelværdierne eller "centroids". Man tilknytter alle observationer til en af de tre clusters (efter de nærmeste af de tre centroids), og så beregner en ny middelværdi/centroid for at hver cluster. Man tilknytter observerationer til den nye cluster centroids og så gentager man processen flere gange.

```{r, echo=FALSE,out.width="50%", fig.cap="source: https://towardsdatascience.com/k-means-a-complete-introduction-1702af9cd8c"}
# Bigger fig.width
library(png)
library(knitr)
include_graphics("plots/kmeans.png")
```

Jeg spørger ikke efter detaljerne i metoden men der er mange videoer på Youtube som bedre foreklarer hvordan k-means fungerer, for eksempel: https://www.youtube.com/watch?v=4b5d3muPQmA

Bemærk, at der er noget __tilfældighed__ indbygget i algoritmen. Det betyder, at hver gang man anvende k-means, få man en lidt anderledes resultat.

<!-- Bare som summary af algoritmen: -->

<!-- * Angiv antallet af clusters - lad os sige 3 her -->
<!-- * Tag 3 tilfældige observationer og kalde dem for de cluster means (centroid i ovenstående figur) -->
<!-- * Tilknytte alle observation til deres nærmeste cluster (de tre centroids) -->
<!-- * Beregne de cluster means fra de 3 sæt observationer. -->
<!-- * Gentage ovenstående steps mange gange indtil de cluster means ændre sig ikke meget.  -->

<!-- I det sidste step vil det sige, at de total __within sum of square er minimized__ - det betyder den totale afstand af observationerne til de centroids er så lav som de kal være, og samtidig at de __between sum of squares er maximized__ - der er så store afstand imellem de tre grupper som der kan være. -->


### Run k-means i R

K-means fungerer kun på numeriske data, som vi kan vælge fra datasættet med `select` - man kan specificere `where(is.numeric)` indenfor `select()` for at slippe for at manuelt indtaste de relevante variable navne. Vi bruger også `scale()` på de data her. Det betyder, at alle variabler få den samme skala og det undgår, at der er nogle som få mere indflydelse end andre i de færdig resultater.

```{r}
penguins_scaled <- penguins %>% 
  select(where(is.numeric)) %>% 
  scale()
```

Man er også nødt til at fortælle i forvejen hvor mange clusters at opdele datasættet ind i, så lad os sige `centers=3` indenfor funktionen `kmeans` her og beregner vores clusters:

```{r}
kclust <- kmeans(penguins_scaled,centers = 3)
kclust
```

Man få forskellige ting frem, for eksempel:

* `Cluster means` - de svarer til de centroids markerede med __x__ i den ovenstående figur - bemærk at her er de 5-dimensionel da vi brugt 5 variabler til at beregne resultatet. 
* `Clustering` vector - hvilke cluster er hver observation blevet tilknyttet til.
* `Within cluster sum of squares` - Jo mindre, jo bedre - hvor meget observationerne indenfor samme cluster ligner hinanden (den totale squared afstand af observationerne fra deres nærmeste centroid).

### Tidy up k-means resultaterne med pakken `broom`

Fra pakken `broom` har vi beskæftiget os med `glance` som vi benyttede til at få enkel-linje baserede summary statistics fra flere modeller sammen i én dataramme, for at facilitete et plot/labels osv. Der er også to andre funktioner vi tager i bruge her. Her er en beskrivelse af de tre.

Funktion    | Beskrivelse
----------- | -----------------
`glance`    | single line summary
`augment`   | Connect information from the model to the original dataset
`tidy`      | multi-line summary

For at lave et plot af de clusters kan det være nyttigt at benytte `augment`. Her kan man se, at vi har fået en kolon der hedder `.cluster` med i den oprindelige dataramme (jeg flyttet kolonen til første plads i følgende kode så man kunne se den i de output af kursusnotater). 

```{r}
kc1 <- augment(kclust, penguins) #clustering = første plads, data = anden plads
kc1 %>% select(.cluster,all_of(names(penguins)))
```

Nu lad os benytte vores datasæt som vi har fået med `augment` til at lave et plot. Her giver jeg en farve efter `.cluster` og shape efter `species` så at vi kan sammenligne vores beregnet clusters med de tre forskellige species. Bemæk her, at jeg kun har to variabler i plottet, men der er faktisk fire variabler som blev brugt til at lave de clusters i med funktionen `kmeans`. En anden måde er at plotte de først to principal components i stedet for to af de fire variabler - det beskæftige vi os med næste gang.

```{r,fig.width=5,fig.height=4}
ggplot(kc1, aes(x = scale(bill_length_mm), 
                y = scale(bill_depth_mm))) + 
  geom_point(aes(color = .cluster, shape = species)) + theme_minimal()
```

Vi kan også fk. tælle op hvor mange af tre species vi få i hver af vores tre clusters, hvor vi kan se, at `Adelie` og `Chinstrap` er blevet mere blandet blandt to af de tre clusters end `Gentoo`.

```{r}
kc1 %>%
  count(.cluster, species)
```

Lad os også kigge på resultatet af `tidy`. Her har vi fået en pæn dataramme med middelværdierne (centroids) af de tre clusters over de fire variabler som blev brugt i beregningerne.

```{r}
kclust_tidy <- tidy(kclust) 
kclust_tidy
```

Lad os benytte `kclust_tidy` som et datasæt i vores ovenstående plot indenfor en anden `geom_point()` til at tilføje en `x` form i de centre af de tre clusters (bemærk jeg har brugt `color` og `shape` som lokale aethetics i den første `geom_point()` her, der de ikke eksistere som kolonner i `kclust_tidy`):

```{r,fig.width=5,fig.height=4}
ggplot(kc1, aes(x = scale(bill_length_mm), 
                y = scale(bill_depth_mm))) + 
  geom_point(aes(color = .cluster, shape = species)) +
  geom_point(data = kclust_tidy, 
             size = 10, shape = "x", show.legend = FALSE) + 
  theme_bw()
```

Vi kan se at vores clusters fanger ikke de samme gruppe som `species` perfekt - der er forskelligheder. Det kan være at vi også har fanget nogle oplysninger om fk. øen de kommer fra eller kønnet af pingvinerne.

## Hvor mange clusters skal der være?

Vi gættede på 3 clusters i ovenstående analyse (da vi havde oplysninger om de species i forvejen) men det kunne være, at et andet antal clusters passer bedre med de data. Vi kan anvende vores statistikker over de forskellige antal cluster til at tage en beslutning om, hvor mange clusters vi gerne vil beholde i vores færdig clustering resultat. Det er vigtigt at kan finde frem til en hensigtsmæssigt antal clusters - for mange clusters kan resultatere i over-fitting, hvor vi har for mange til at fortolke eller giver mening, og for få kan betyde at vi mangler indsigter ind i strukturen eller trends i de data.

### Få statistikker for antal clusters fra 1 til 9

I nedenstående iterater vi over vectoren `1:9`, som vi angive i `kmeans` for at fortæl hvor mange clusters vi gerne vil beregne. For hver af de integar 1 til 9, benytter vi således `kmeans` med hjælp af funktionen `my_func` (bemærk at vores input data `.x` er antallet af clusters men de datasæt er den samme hver gang). Dernæst benytte vi `tidy`, `glance` og `augment` på vores resultaterne fra `kmeans`:

```{r}
my_func <- ~kmeans(penguins %>% select(where(is.numeric)) %>% scale(),
                 centers = .x)  

kclusts <- 
  tibble(k = 1:9) %>%
  mutate( kclust = map(k, my_func),
          tidied = map(kclust, tidy),
          glanced = map(kclust, glance),
          augmented = map(kclust, augment, penguins)
        )
```

Husk at for at få frem resultaterne i de forskellige former fra `tidy`,`glance` og `augment` er vi nødt til at anvende `unnest`:

```{r}
kclusts_tidy    <- kclusts %>% unnest(cols = c(tidied))
kclusts_augment <- kclusts %>% unnest(cols = c(augmented))
kclusts_glance <- kclusts %>% unnest(cols = c(glanced))
```


### Manuelt beslutning med elbow plot.

Vi bruger `tot.withinness` fra vores `glance` output. Det måler hvor meget observationerne indenfor samme cluster ligner hinanden og er den totale afstand af observationerne fra deres nærmeste centroid. Jo flere clusters, jo mindre statistikken, men vi kan se, at efter 2-3 clusters, er der ikke meget gevinst med at bruge flere clusters. Derfor vælger man enten 2-3. Der er ofte kaldes for en 'elbow' plot - man vælge de tal på den 'elbow', hvor der ikke er meget gevinst med at have flere clusters i de data.

```{r,fig.width=5,fig.height=3.5}
kclusts_glance %>% 
  ggplot(aes(x = k, y = tot.withinss)) + 
  geom_line() + 
  geom_point() + 
  theme_bw()
```



### Automatistke beslutning med pakken `NbClust`

Hvis man synes at de er svært at vælge et bestemt cluster tal fra de elbow plot, kan man også prøve noget mere automatisk. For eksempel pakken NbClust lave 30 forskellige clustering algoritme af de data fra antal clusters = 2 til antal cluster = 9 og for hver af de 30 tage en beslutning om de bedste antal clusters. Man kan således se hvilket antal clusters blev valgt af de mest algoritmer.

```{r,comment=FALSE,message=FALSE,fig.show='hide',results='hide'}
library(NbClust)
cluster_30_indexes <- NbClust(data = penguins %>% select(where(is.numeric)) %>% scale, 
                              distance = "euclidean", 
                              min.nc = 2, 
                              max.nc = 9, 
                              method = "complete")
```

Man kan se her, at enten 2 eller 3 er optimelt, som passer sammen med den elbow plot methode.

```{r,fig.width=4,fig.height=2.5}
as_tibble(cluster_30_indexes$Best.nc[1,]) %>%
  ggplot(aes(x=factor(value))) + 
  geom_bar(stat="count",fill="blue") + 
  coord_flip() +
  theme_minimal()
```

### Plot de forskellige antal clusters

Vi kan også visualisere hvordan de forskellige antal clusters ser ud. Her kan vi bruge vores resultater fra funktionen `augment` til hver af de 9 clusterings. Her har vi har plottet `flipper_length_mm` vs `bill_length_mm`.

```{r}
kclusts_augment %>% 
  ggplot(aes(x = flipper_length_mm, y = bill_length_mm,colour=.cluster)) +
        geom_point(aes(shape=factor(species)), alpha = 0.8) + 
        facet_wrap(~ k) + theme_bw()
```

Her introducerer jeg `sex` som en ekstra variable i de plot. Husk at variablen `sex` ikke blev brugt i k-means, men det kan være, at der er nogle aspekter af de fire variabler, som kan fortælle os nogle om kønnet af pingvingerne. For at spare plads, har jeg kun plottet antal clusters fra 2 til 5.

```{r,fig.width=10,fig.height=6}
kclusts_augment %>% filter(k %in% 2:5) %>% 
  ggplot(aes(x = scale(flipper_length_mm), y = scale(bill_length_mm),colour=.cluster)) +
        geom_point(aes(shape=factor(species)), alpha = 0.8) + 
        facet_wrap(~ sex+k,nrow=2) + theme_bw()
```



## Hierarchical clustering

K-means er en meget populær måde at lave clusters i de data på, men der er mange andre metoder til at lave clusters. Jeg nævne kort hierarchical clustering. Derfor vil jeg også navne her hierarchical clustering. Vi skifter over til `mtcars`, og ligesom i `kmeans` skal vi bruge `scale` på de numeriske kolonner i de data.

```{r}
mtcars_scaled <- mtcars %>% select(where(is.numeric)) %>% scale
```

I modsætning til k-means, for at lave hierarchical clustering skal man første beregne afstanden mellem alle de observationer i de data. De gøre man med funktionen `dist` (som bruger den Euclidean distance som default):

```{r}
d <- dist(mtcars_scaled)
```

For at lave en hierarchical clustering anvender man funktionen `hclust`. Metoden `complete` er default men man kan afprøve de andre methoder (der er ikke en fast regel over for, hvilken man skal bruge).

```{r}
mtcars_hc <- hclust(d, method = "complete" )
# Metoder: "average", "single", "complete", "ward.D"
```

I følgende arbejder vi lidt med `mtcars_hc` til at få nogle clusters frem, og til at lave et plot.

### Vælge ønkset antal clusters

Funktionen `cutree` anvendes til at få clusters fra de data. For eksempel, hvis man gerne vil have 4 clusters, bruger man `k = 4`. Jeg specificerer `order_clusters_as_data = FALSE` for at få de clusters i de rækkefølger, som passer til plottet (dendrogram) vi lave (man skal have ovenstående pakker installeret for at få den til at fungere).

```{r,comment=FALSE,message=FALSE,warning=FALSE}
library(dendextend)

clusters <- cutree(mtcars_hc, k = 4, order_clusters_as_data = FALSE)

tibble("cluster"=clusters) %>% group_by(cluster) %>% summarise(n())
```

### Lav et pænt plot af dendrogram med ggplot2

Første vi anvende `dendro_data` til at udtrække de dendrogram oplysninger fra de `hclust` resultater.

```{r,comment=FALSE,message=FALSE,warning=FALSE}
library(ggdendro)
dend_data <- dendro_data(mtcars_hc %>% as.dendrogram, type = "rectangle")
```

Vi tilføjer vores clusters som vi beregnede ovenpå (det er derfor vi sikrede rækkefølgen af de clusters):

```{r}
dend_data$labels <- dend_data$labels %>% 
  mutate(cluster = clusters)
```

Vi benytter `dend_data$segments` og `dend_data$labels` til at lave et informativ plot af de data i `ggplot2`.

```{r}
ggplot(dend_data$segments) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  coord_flip() +
  geom_text(data = dend_data$labels, 
            aes(x, y, label = label,col=factor(cluster)),
            hjust=1,size=3) +
  ylim(-3, 10) + 
  theme_dendro()
```


### Ekstra: afprøve andre metoder

Her giver jeg nogle valgfri ekstra kode til at afprøve de fire metoder - "average", "single", "complete" og "ward.D".

```{r,fig.width=12,fig.height=10}
# samme ggplot kommando som ovenpå lavet til en funktion 
den_plot <- ~ggplot(.x$segments) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  coord_flip() +
  geom_text(data = .x$labels, 
            aes(x, y, label = label),
            hjust=1,size=2) +
  ylim(-4, 10) + theme_dendro()
```

Vi iterate over de fire metoder og lave samme process som ovenpå med map. Derefter kan man lave et plot fk. med grid.arrange:

```{r,message=FALSE,comment=FALSE,warning=FALSE}
# fire metoder:
m <- c( "average", "single", "complete", "ward.D")

hc_results <- 
  tibble(method = m) %>%
  mutate( kclust = map(method, ~hclust(d, method = .x)), 
          dendrogram = map(kclust,as.dendrogram),
          den_dat = map(dendrogram,~dendro_data(.x,type="rectangle")),
          plot = map(den_dat,den_plot))

library(gridExtra)
grid.arrange(grobs = hc_results %>% pull(plot),ncol=2)
```


## Problemstillinger

__0__) Quiz - Clustering

__1__) *Funktionen kmeans*. I ovenstående anvendt vi `mtcars` i hierarchical clustering, men lad os se, hvordan det ser ud med `k-means`. Man kan tilpasse den ovenstående kode for det `penguins` datasæt:

 * __a__) Benyt `kmeans` til at finde 2 clusters i de data 
    + husk at vælge kun de numeriske kolonner og scale de data i forvejen
    + gem din clustering som `my_clusters`.
    + hvor mange observationer er der i hver af de to clusters?
    
```{r,eval=FALSE,echo=FALSE}
data(mtcars)
my_clusters <- mtcars %>% select(where(is.numeric)) %>% scale %>%
  kmeans(centers=2)
my_clusters
```

 * __b__) Anvend `augment` til at forbinde de datasæt til de clusters fra `my_clusters` (husk at din clustering resultat skrives i første plads i funktionen og så de data i anden plads).

```{r,eval=FALSE,echo=FALSE}
my_clusters_augment <- augment(my_clusters,mtcars)
my_clusters_augment
```

 * __c__) Brug din augmented datasæt til at lave et scatter plot mellem to af de numeriske variabler i de data og give dem farver efter de clusters du har beregnet.
    + hvis du har forbundet det oprindeligt data (der ikke var scaled) i `augment`, husk at scale de data i plottet.
    + prøve også to andre numeriske variabler
 
```{r,eval=FALSE,echo=FALSE}
my_clusters_augment %>% 
  ggplot(aes(scale(mpg),scale(wt))) +
  geom_point(aes(colour=.cluster)) +
  theme_minimal()
```


 * __d__) Anvend `tidy` til at finde ud af de middelværdier/centroids af hver af de 2 clusters
    + tilpas min kode fra notaterne til at tilføj dem til plottet som 'x'.
 
```{r,eval=FALSE,echo=FALSE}
my_clusters_tidy <- tidy(my_clusters)
```
 

```{r,fig.width=5,fig.height=4,eval=FALSE,echo=FALSE}
my_clusters_augment %>% 
  ggplot(aes(scale(mpg),scale(wt))) +
  geom_point(aes(colour=.cluster)) + 
  geom_point(data=my_clusters_tidy,col="black",shape="X",size=6) +
  theme_minimal()
```

__2__) *k-means for forskellige k*

Åbn `LungCapData` fra nedenstående link

```{r}
LungCapData <- read.csv("https://www.dropbox.com/s/ke27fs5d37ks1hm/LungCapData.csv?dl=1")
LungCapData <- as_tibble(LungCapData)
head(LungCapData) #se variabler navne
```

  * __a__) Anvend `kmeans` på `LungCapData`
    + Vælg de numeriske variabler og scale
    + Angiv `centers = 3`
    + Benytte `augment` til at forbinde resultaterne til `LungCapData` og gemme resultatet.
    
```{r,eval=FALSE,echo=FALSE}
my_clusters <- LungCapData %>% select(where(is.numeric)) %>% scale %>%
  kmeans(centers=3)

my_clusters_augment <- augment(my_clusters,LungCapData)
```

  * __b__) Lav et scatter plot mellem `LungCap` og `Age` og giv farver efter din beregnet clusters.

```{r,eval=FALSE,echo=FALSE}
my_clusters_augment %>% 
  ggplot(aes(scale(LungCap),scale(Age))) +
  geom_point(aes(colour=.cluster)) + 
  theme_minimal()
```

  * __c__) Lav et scatter plot mellem `LungCap` og `Height` og giv farver efter `Smoke`.
    + Angiv en forskellige form efter `Smoke`

```{r,eval=FALSE,echo=FALSE}
my_clusters_augment %>% 
  ggplot(aes(y = scale(LungCap),scale(Height))) +
  geom_point(aes(colour=.cluster, shape = `Smoke`)) + 
  theme_minimal()
```

 * __d__) Man kan også bruge `my_clusters_augment` til at beregne middelværdier med `group_by` og `summarise`.
    + group efter `.cluster` og `smoke` og beregner den gennemsnitlige `LungCap` og `Height` for hver kombination.
    + Angiv datarammen af resultaterne middelværdier indenfor plottet med `geom_point` for at få dem som "X" punkter i plottet.

```{r,eval=FALSE,echo=FALSE}
means_by_smoke <- my_clusters_augment %>% 
  group_by(.cluster, Smoke) %>% 
  summarise(LungCap = mean(LungCap),
            Height = mean(Height))
```

```{r,fig.width=5,fig.height=4,eval=FALSE,echo=FALSE}
my_clusters_augment %>% 
  ggplot(aes(y = scale(LungCap),x = scale(Height))) +
  geom_point(aes(colour=Smoke, shape = Smoke)) + 
  geom_point(data=means_by_smoke,shape="X",color="black", size=6) +
  theme_minimal()
```

Lad os undersøge om, 3 var en gode antal clusters til at beregne i de data.

  * __d__) Tilpas koden nedenstående (baserende på den jeg præsenterede i notaterne) til at beregne 9 clusterings for det `LungCapData` datasæt.

```{r,eval=FALSE}
my_func <- ~kmeans(??? %>% select(???) %>% ???,
                 centers = ???)  

kclusts <- 
  tibble(k = ???) %>%
  mutate( kclust = map(k, my_func),
          tidied = ???,
          glanced = ???,
          augmented = ???
        )


kclusts_tidy    <- kclusts %>% unnest(???)
kclusts_augment <- ??? 
kclusts_glance <-  ???
```


```{r,eval=FALSE,echo=FALSE}
my_func <- ~kmeans(LungCapData %>% select(where(is.numeric)) %>% scale,
                 centers = .x)  

kclusts <- 
  tibble(k = 1:9) %>%
  mutate( kclust = map(k, my_func),
          tidied = map(kclust, tidy),
          glanced = map(kclust,glance),
          augmented = map(kclust,~augment(.x,LungCapData))
        )

kclusts_tidy    <- kclusts %>% unnest(cols = c(tidied))
kclusts_augment <- kclusts %>% unnest(cols = c(augmented))
kclusts_glance <- kclusts %>% unnest(cols = c(glanced))
```


  * __e__) Lav et "elbow"-plot fra din beregnet clusterings
    + Anvend `kclusts_glance` og plot `tot.withinss` på y-aksen.

```{r,fig.width=5,fig.height=4,eval=FALSE,echo=FALSE}
kclusts_glance %>% 
  ggplot(aes(x = k, y = tot.withinss)) + 
  geom_line() + geom_point() + theme_bw()
```

  * __e__) Afpøv automatiske method (tilpas min kode fra kursus notaterne)


```{r,comment=FALSE,message=FALSE,fig.show='hide',results='hide',eval=FALSE,echo=FALSE}
library(NbClust)
cluster_30_indexes <- NbClust(data = LungCapData %>% select(where(is.numeric)) %>% scale, 
                              distance = "euclidean", 
                              min.nc = 2, 
                              max.nc = 9, 
                              method = "complete")
```


```{r,fig.width=4,fig.height=2.5,eval=FALSE,echo=FALSE}
as_tibble(cluster_30_indexes$Best.nc[1,]) %>%
  ggplot(aes(x=factor(value))) + 
  geom_bar(stat="count",fill="blue") + 
  coord_flip() +
  theme_minimal()
```

  * __f__) Visualiser de forskellige antal clusters i et plot
    + Tilpas min kode og benytter `kclusts_augment`
    + Husk at bruge `facet_wrap` til at adskille efter k.

```{r,eval=FALSE,echo=FALSE}
kclusts_augment %>% 
  ggplot(aes(y = scale(LungCap),x = scale(Height))) +
  geom_point(aes(colour=.cluster, shape = Smoke)) + 
  theme_minimal() + facet_wrap(~Smoke+factor(k),ncol=9)
```

__3__) *Hierarchical clustering øvelse*

Vi laver en analyse af det `msleep` datasæt. Jeg har lavet oprydningen og scaling for jer:

```{r}
data(msleep)
msleep_clean <- msleep %>% select(name,where(is.numeric)) %>% drop_na()
msleep_scaled <- msleep_clean %>% select(-name) %>% scale
row.names(msleep_scaled) <- msleep_clean$name
```


Tilpas min kode fra kursusnotaterne til at lave følgende skridt:

 * __a__)  Benyt funktioner `dist` og så `hclust` på datasættet `msleep_scaled`. 


```{r,eval=FALSE,echo=FALSE}
msleep_scaled_dist <- dist(msleep_scaled)
hc <- hclust(msleep_scaled_dist)
```

* __b__) Benyt `cuttree` for at finde 5 clusters i de data, og kalde det for `clusters`. Husk at anvende `order_clusters_as_data = FALSE` så at vi har den korrekt rækkefølge for et plot (man skal indlæse pakken `dendextend`)

```{r,eval=FALSE,echo=FALSE}
clusters <- cutree(hc, k = 6, order_clusters_as_data = FALSE)
```

* __c__) Benyt `dendro_data` til at udtrække de dendrogram fra resultaterne
    + Tilføj `clusters` til `dend_data$labels`

```{r,eval=FALSE,echo=FALSE}
dend_data <- dendro_data(hc %>% as.dendrogram, type = "rectangle")
```

```{r,eval=FALSE,echo=FALSE}
dend_data$labels <- dend_data$labels %>% 
  mutate(cluster = clusters)
```

* __d__) Lav et plot af de dendrogram
  + Tilpas koden for `mtcars` eksempel for nuværende data


```{r,eval=FALSE,echo=FALSE}
ggplot(dend_data$segments) + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  coord_flip() +
  geom_text(data = dend_data$labels, 
            aes(x, y, label = label,col=factor(cluster)),
            hjust=1,size=3) +
  ylim(-3, 10) + 
  theme_dendro()
```


